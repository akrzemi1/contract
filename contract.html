<html>
<head></head>
<body>


<h1>What do we need?</h1>

<p>
The 'features' we describe here can be collectively called <em>value constraints.</em>
We want to use them to express in the language that certain combinations of values of certain objects 
at certain times are invalid and are intended (and expected) never to occur.</p>


<h2>Specify function's domain</h2>

<p>The problem goes back to mathematics. Certain functions are well defined
only for a subset of values of the input type. for instance a square root
over real numbers is not defined for negative numbers. 
What does it mean for a function in a programming language like C++ 
that it is not defined? Currently there are two approaches. One is to detect 
the value of the parameter(s) that the function should not be prepared for 
and execute a different logic: returning a special value, throwing an exception, etc. 
Using our example with a square root, a corresponding function <code>sqrt()</code>
could be defined as follows:</p>

<pre>double sqrt(double x)
{
  if (x &gt;= 0.0) {
    //<em> do proper algorithm</em>
  }
  else {
    return numeric_limits&lt;double&gt;::signaling_NaN();
    // <em>or throw an exception</em>
  }
}
</pre>

<p>What it effectively does is to change the function's domain. Now the functions do not restrict its domain anymore.
It is well defined for every value of an input type, and does other things than only the "proper algorithm". 
This has an unintended consequence: our function can be used for detecting negative numbers:
</p>

<pre>double is_negative(double x)
{
  return isnan(sqrt(x));
}
</pre>

<p>Another way of approaching the function domain problem is to informally 'announce' that the function is not
defined for certain values of input types and implement it with the assumption that the undesired values
are never passed to the function:</p>

<pre>
double sqrt(double x)
// requires: x >= 0.0
{
  // proper algorithm:
  double y = 1.0;
  double prev_y;
 
  do {
    prev_y = y;
    y = (y + x / y) * 0.5;
  }
  while (!closeEnough(y, prev_y));
 
  return y;
}
</pre>

<p>This uses Newton's Iteration algorithm. The loop will terminate when values <code>y</code> and <code>prev_y</code> become close enough. They will only do that when <code>x</code> is non-negative. Supplying a negative value will make the loop run forever and hang the program. Even a clever analyser is unlikely to detect from the function body that passing a negative input to this function is something wrong. But everything is fine as long as no negative number is passed in. 'Announcing' that the function is not prepared for just any input is informal: it is either written in the comment, or in paper or html documentation, or only spread by word, or assumed to be obvious. This risks an unintended negative program behaviour, like a halt, or a UB. However, it is often preferred because of performance and inability to come up with a reasonable fall-back action. Consider <code>std::vector::operator[]</code>, it does not check the bounds for performance reasons. If it was used in the following code:</p>

<pre>
for (int i = 0; i != vec.size(); ++i) {
  if (i &gt; 0) cout &lt;&lt; ", ";
  cout &lt;&lt; vec[i];
}
</pre>

<p>The condition in the loop already checks if the index is within bounds. If <code>operator[]</code> was also checking the same condition inside, we would be wasting time on doing redundant checks. Also, for some functions it is not possible to think of any fall-back action. For instance <code>std::vector::swap</code> must not throw exceptions and returns <code>void</code>, yet, it is not well defined when two vectors' allocators do not compare equal.</p>

<p>Not checking for argument being within domain and delegating the check to the callers is desirable, especially that the caller is in a better position in verifying if the domain of the function. Consider:</p>

<pre>return sqrt(abs(x));</pre>

<p>The caller can be sure that no negative value is passed to function <code>sqrt()</code> even though no check is performed at all. Function domain defined this way is a <em>contract</em> between function author and function callers. The author defines the domain and assumes he will not get values from outside the domain. The caller guarantees that she will not pass values outside the domain. Out of all contracts described here, this one is the most likely to be broken, because it is the only one where the person responsible for guaranteeing is different than the person declaring the contract. The user may not be aware that a function has requirements on the values of parameters, or may misunderstand what the requirement is. Therefore this contract requires the most serious attention and support in the language.</p>

<p>What we need is the ability to declare the function's domain along with the function, so that it is visible to the callers and to the automated tools. How the tools can handle this is a secondary issue, but the behaviour can be: (1) automated documentation generation, (2) additional warnings from static analysers, (3) additional instrumentation injected by compilers, (4) compiler optimizations based on domain declarations.</p>

<h2>Specify function's co-domain</h2>

<p>In the above example with function <code>abs()</code> a hypothetical tool could detect that expression <code>sqrt(abs(x))</code> is always fine only if it knew that the co-domain of <code>abs</code> is equal (or smaller) than the domain of <code>sqrt</code>. We need to be able to constrain the allowed function output in order for automated tools to be able to verify if other functions' domain requirements are satisfied. </p>

<p>In this case a function author specifies the contract and (likely) the same author guarantees it. Function's user can rely on the guarantee. It is much less likely that this contract is broken, because it is the same person that declares and later ensures the obligation. And conversely, if the author makes an error in the function, he is equally likely to make an error in specifying the contract.</p>


<h2>Block-level assertions</h2>

<p>This type of value constraint is quite familiar to many C++ user, as it is implemented with macro <code>assert</code> and some similar tools in many libraries. If we think of it as a contract, this time a function author defines it, he ensures that it holds, and he is the benefactor of the contract: he guarantees something to himself, namely that certain state of a block-local variable or set of variables shall never occur at the point of the assertion. At the same time the function author guarantees something that he can be held accountable for, and relies on the guarantee expressed with the assertion.</p>

<h2>Class-level assertions</h2>

<p>A similar constraint on variables is often required for (non-static) data members of objects of a given class. Consider the following example:</p>

<pre>class WeighedAverage
{
  double wgt1_ = 0.5;
  double wgt2_ = 0.5;
public:
  double average(double val1, double val2) const; 
  <em>// ensures: answer is between val1 and val2</em>
  { return wgt1_ * val1 + wgt2_ * val2; }

  void set_1st_weight(double w1); 
  <em>// requires: w1 between 0.0 and 1.0</em>
  { wgt1_ = w1; wgt2_ = 1.0 - w1; }

  <em>// always true: wgt1_ between 0.0 and 1.0</em>
  <em>// always true: wgt2_ between 0.0 and 1.0</em>
  <em>// always true: wgt1_ + wgt2_ == 1.0</em>
};
</pre>

<p>Note the three comments at the bottom. They are something that could be expressed with macro <code>assert</code>, except that <code>assert</code> is an expression and cannot be put at the class scope. Note also that members <code>wgt1_</code> and <code>wgt2_</code> are private. So, we are not declaring anything to the outside world. Word "always" needs to be more formal. It applies to any object of this class whose life time has begun and have not yet ended, except that member functions can temporarily compromise it provided that they restore it upon exit (either via exception or a normal return). Also, in case of thread or context switching, if a function decides to temporarily break such class-scope assertion, it must guard this action with a lock or equivalent, so that other threads do not see an object with a broken contract.</p>


<h2>Specify equivalent expressions</h2>


<p>Often library interfaces offer more than one way of doing the same thing. For instance, checking if a container contains at least one element can be checked in two ways: <code>cont.empty()</code> and <code>cont.size() != 0</code>. The latter is more general and could be used to check for emptiness, but there are occasions where the former can perform faster. This justifies the existence of the two forms. However, the job of matching one function's domain with other function's co-domain would be severely impeded if either specified the contract with a different (albeit equivalent) expression. consider:</p>

<pre>template &lt;typename T&gt;
class vector
{
  void resize (size_t s); <em>// ensures: this->size() == s</em>
  const T&amp; front() const; <em>// requires: !this->empty()</em>
};

vec.resize(1);
return vec.front();       <em>// safe?</em>
</pre>

<p>Can we match what <code>resize</code> guarantees with what <code>front</code> expects? The two expressions are different. It would be much easier if we were able to explicitly declare that for any container expressions <code>cont.empty()</code> and <code>cont.size() != 0</code> are equivalent. This is somewhat similar to class-level assertions, but the declaration applies to more than one class, now we are only interested in the public interfaces, and we can express equivalence between expressions that have side effects. for instance, we can specify that <code>cont.clear()</code> is equivalent to <code>cont.resize(0)</code>. Out of all "constraints" described in this section, this is the only one where term "value constraint" is inadequate. This is a constraint on the semantics of expressions.</p>

<h2>Loop variants</h2>

<p>This is somewhat similar to an assertion. It can help convince oneself that a non-trivial loop will ultimately terminate. Sometimes it is not easily seen, because there is no loop counter. We may for instance inspect elements in the collection by skipping some elements, increasing or shrinking, the collection. A loop variant is an expression that evaluates to an integral non-negative number. It is expected that in each loop iteration evaluating this expression renders a number smaller than in the previous iteration. It is also expected that when the value renders zero, the loop terminates.</p>

<p>A framework for supporting value constraints need not address all the above needs. We believe that only supporting the declaration of function domain is a big help in itself. Note that while assertions loop variants need to work with expressions, the features for specifying function domain and co-domain need not necessarily use expressions. We address this in detail in the later sections.</p>



<h1>What use can be made of value constraints?</h1>

<p>Value constraints exist, even though there is no language support for them in the language. We have seen in the above examples that comments were used to convey the information. In this section we list the benefits obtained form standardizing or formalizing value constraints this way or the other.</p>

<p>In order for an automated tool to understand the constraint, we have to provide a special-purpose syntax. Some tools expect that it is put inside source code comments. For instance [ACSL] uses the following notation: </p>
<pre><em>/*@ requires x >= 0;
    ensures \result >= 0; 
*/</em>
double sqrt(double x);</pre>

<p>But for structured comments like the above, C++ has an alternative: [[attributes]]. Attributes are akin to comments because they cannot affect the semantics of the program. On the other hand they can be used as hints for optimizations, warnings and any kind of program analysis. If the expectation of value constraints is to affect the program behaviour, attributes will not do, and a special language feature would need to be devised.</p> 

<p>An ideal &mdash; and unrealistic &mdash; support for value constraints is for compiler to check any possible flow in the program and signal a compiler error whenever a breach of the constraint is going to happen. Because this is not doable, the we aim at the support that is not entirely satisfactory, but believed to be better than none.</p>

<h2>Improved documentation</h2>

<p>One of the obvious uses of formalized value constraints is the automated generation of documentation. Even if developers do not use any tool for generating documentation, there is still a gain. When a developer sees the declaration of the function she is going to use, she can immediately see the value constraints along. If they are a language feature, developers are more encouraged to use them (even though comments would do). Since this does not affect program semantics [[attributes]] are sufficient.</p>

<h2>Static analysis</h2>

<p>Value constraints could enable static analysers to detect a potential breach of value constraints and issue warnings. Again, using value constraints this way does not affect program semantics, so [[attributes]] would do.</p>

<h2>Constraint-based compiler optimizations</h2>

<p>In that case, compiler is able to arbitrarily change the behaviour of the program in case where a value constraint has been violated. So, adding a value constraint may change the meaning of the program. In this case attributes will not suffice, and we need a language extension for specifying value constraints.</p>

<h2>Auto-generation of runtime checks</h2>

<p>This is what Eiffel provides and what [N1962] proposes. Value constraints need to be C++ expressions. An implementation can (optionally) inject, at well defined places, a code that evaluates at run-time expressions representing value constraints. If any (boolean) expression evaluates to <code>false</code>, a certain action (like calling <code>std::terminate</code>) is performed. The insertion of additional logic requires that value constraints are introduced as a language feature. In fact even more language and library features is required to control when and how the run-time checks are performed and responded to.</p>


<h1>Run-time evaluation of value constraints</h1>

<p>In this section we focus on one possible approach: treating value constraints as expressions and evaluating them at run-time.</p>

<h2>Side effects of the expressions</h2>

<p>Using expressions with side effects in value constraints is generally discouraged, but sometimes it might be difficult to spot that we have a side effect. For the purpose of this discussion we also consider run-time overhead, and especially the run-time complexity as a side effect. To minimize the possibility of invoking a function with a side effect [N1962] proposes that only <code>const</code>-qualified member functions are allowed; but even these can modify non-member variables, and it is not only member functions that may need to be used to express value constraints. Ideally, we would like to use only <em>pure</em> (referentially transparent) expressions, but C++ as of today does not offer the possibility of detecting if an expression is pure. Although the definition of relaxed <code>constexpr</code> functions makes a step towards pure functions.</p>

<p>A practical question to be answered, given the syntax from [N1962], is if the following to be a legal code for detecting if run-time value constraint checks are enabled?</p>

<pre>
int preconditions_on = false;
void test() precondition{ preconditions_on = true; }
{
  cout &lt;&lt; "preconditions are on: " &lt;&lt; preconditions_on;
}
</pre>

<p>And similarly, is the following a reliable way of checking if a number is negative?</p>

<pre>double sqrt(double x) precondition { x >= 0.0; };

double is_negative(double x)
{
  set_precondition_broken_handler(&amp;throw_exception);
  try { sqrt(x) };
  catch (exception const&amp;) { return true; }
  return false;
}
</pre>

<h2>Value constraints in overloaded functions</h2>

<p>Consider the following declaration:</p>

<pre>
template &lt;typename IIT&gt; <em>// requires: InputIterator&lt;IIT&gt;</em>
void displayFirstSecondNext(IIT beg, IIT end);
<em>// requires: std::distance(beg, end) &gt;=2</em>
</pre>

<p>Is function template <code>std::distance</code> referentially transparent? The answer is: it is not templates that can or cannot be pure but functions. Some functions instantiated from this template will be pure, others will not &mdash; it depends what iterator type the template will be instantiated with. Consider <code>InputIterator</code>. In the worst case (of <code>std::istream_iterator</code>) incrementing the iterator invalidates other iterators referring to the same stream. This is a tricky behaviour: by changing our internal copies of objects (iterators), we alter (invalidate) other, external objects. Function <code>std::distance</code> does increment iterators. If our precondition was to be evaluated, this might cause a UB in the program.</p>

<p>Thus, we have an expression; it is pure and natural to use for some instantiations, and has severe side effects for other instantiations. If we want it to be evaluated only in certain instantiations, another feature needs to be provided. For instance, we will need to specialize a template only to be able to specify a different precondition, or add an annotation that this precondition should not be evaluated if certain compile-time predicate evaluates to <code>true</code>. clearly, on global switch (similar to <code>NDEBUG</code>) saying "disable/enable all value constraint evaluations" will not be enough.</p>

<h2>Inadvertent increase in run-time complexity</h2>
<p>There are other reasons for having a more fine grained control of which checks should be enabled. Consider a binary search algorithm. Its runtime complexity is O(log <var>n</var>). It has an obvious precondition that the range we search through is sorted. Performing the sorted check has complexity O(<var>n</var>). Thus by evaluating the precondition, we change the algorithms complexity. This might be unacceptable, even for some debug/testing builds. Runtime complexity is a sort of side effect of an algorithm, and it can be silently added to the function, especially when following a good practise a developer adds an obvious precondition, which would be his primary task, and forgets the secondary task of specifying (using some contract-specific sub-language) the conditions under which this precondition shall be evaluated.</p>

<h2>Run-time response to a broken contract</h2>

<p>[N1962] proposes that the action taken upon violating a value constraint should be configurable by the programmer. While we agree this should be configurable, we object to the way of achieving this by <code>std::set_terminate</code>-like registering function. <code>std::set_terminate</code> can be called multiple times from multiple places: different, possibly dynamically-loaded/shared libraries. This makes sense for <code>std::terminate</code>. Every part of the system may acquire a <em>critical</em> resource: one that has to be released even if "exception handling must be abandoned for less subtle error handling techniques". In that case the component also needs to register an additional clean-up function. In case of broken contract situation, we do not need or want that. We believe only the person that assembles the final program from the components and libraries, should be empowered to to decide how broken contracts are handled. Only him does have the knowledge if this is a test or a retail build; if the slow-down of some run-time checks can be afforded. Libraries do not know that: they do not know in what environment they will be used.</p>

<p>Additionally, the <code>std::set_terminate</code> mechanism has a certain flow. Unless you apply some syntactic contortions, you cannot set the handler for functions executed before <code>main()</code>.</p>

<p>We are not aware of any mechanism currently available in C++ that would facilitate our needs. But since function <code>main</code> has the property of being required to be defined only once across the executable program, a solution we are leaning towards is to apply some special annotation around function <code>main</code> that would indicate what the user's preference is on the handling of broken contracts.</p>


<h1>Assisting static analysis</h1>

<p>Automated tools already perform static analysis, even without any support from the programmer. However, additional hints from developers could enable even further analysis, or enable vendors to add analysis in compilers at lower expense. In the example like the following, a compiler can fairly easily detect that a potential UB is at hand:</p>

<pre>int* produce()
{
  if (cond) return &amp;global;
  else      return nullptr;
}

void consume(int* p)
{
  use(*p);
}

consume(produce());
</pre>

<p>But even here, certain problems occur. What if the two functions are compiled separately; e.g., if <code>produce</code> is part of a separate third party library? Even if compiler performs a per-file analysis, it may not have sufficient resources (RAM) to perform a global link-time analysis. Even if the analysis is successful and detects a potential UB, it doesn't tell us who of the three: author of <code>produce</code>, author of <code>consume</code> or the guy who assembles them, is responsible for the situation and should fix the problem. This is somewhat similar to the situation we face today with template instantiation error messages. Compiler sees that there is a syntax/type error, it can give you the entire context, but it cannot tell at which level of instantiation process the source of the problem lays.</p>

<p>Also, as shown in the example with <code>sqrt</code> it is sometimes not possible to tell from only observing the reads from a variable what the function's requirement is. In the case of <code>sqrt</code> it is not the UB that we want to avoid but an infinite recursion.</p>

<p>Another reason where assisting the static analysis may prove useful is when there is more than one way to express a given condition. Consider the following code:</p>

<pre>
class Vector
{
  const T&amp; front() const; <em>// requires: !this->empty()</em>
  <em>// ...</em>
};

void test(Vector &amp; vec)
{
  if (vec.size() != 0)
    use(vec.front());
}
</pre>

<p>You know that <code>vec.size() != 0</code> and <code>!vec.empty()</code> mean the same thing, but compiler may not, especially if it cannot look inside the definitions of member functions (because they are compiled separately).</p>

<h2>How long does a condition hold?</h2>

<p>We illustrate the issue in question with the following two examples.</p>

<pre>
Iter test(Iter b, Iter e, Value_type&lt;Iter&gt; v)
{
  std::sort(b, e);
  fun();
  std::binary_search(b, e, v); // requires: is_sorted(b, e)
}
</pre>

<p>Assuming that a static analyser can somehow figure out that <code>sort</code> leaves elements in the state that satisfies the condition <code>is_sorted</code>, can it be sure that the condition still holds when <code>fun</code> is finished? After all <code>fun</code> could be manipulating iterators that alias <code>b</code> and <code>e</code>. Apparently, static analysis cannot give us a 100% bug-free guarantee. It can only helps detect certain bugs.</p>

<p>Second, it is obvious that a certain condition does not last forever. How is the analyser supposed to know when a given condition ceases to hold? One easy answer would be to say that when a non-<code>const</code> operation is called on the object all conditions it might  have satisfied are now considered not satisfied. But this would break even simple use cases:</p>

<pre>void test(std::vector&lt;int&gt; &amp; vec)
auto b = vec.begin(); <em>// non-const: invalidates all conditions</em>
auto e = vec.end();   <em>// non-const: invalidates all conditions</em>
std::sort(b, e);      <em>// requires: valid_range(b, e)</em>
</pre>

<p>Even if the analyser knows that <code>vec.begin()</code> and <code>vec.end()</code> form a valid range, they are a non-<code>const</code> operations and invalidate any assumptions about their value/state. Any language feature for assisting static analysis must provide a way of specifying which operations invalidate which conditions. But this is likely to make the specifications very complicated, and discourage everybody from using the feature.</p>


<h1>"Properties": a hypothetical language feature</h1>

<p>In this section we describe a language feature that would enable defining value constraints in a somewhat different way than the "imperative" approach described in [N1962]. We call it <em>properties</em>, which have been described in [N3351]. A property is a new kind of "entity" in the language: it is neither an object, nor a function, it is just something else. It is introduced by an invented keyword "<code>property</code>":</p>

<pre>template &lt;typename T&gt;
property bool is_non_empty (std::vector&lt;T&gt;);
</pre>

<p>A property is something that an object, or a group of objects can acquire at a certain point in time, and then loose at a different point in time. There is no function-body associated with the property: it only has a name. A property can be used in specifying pre- and postconditions:</p>

<pre>
template &lt;typename T&gt;
class vector
{
  void push_back (T const&amp;); // post: is_non_empty(*this);
  const T&amp; back() const; // pre: is_non_empty(*this);
};

vector&lt;int&gt; v;  // doesn't have property is_non_empty yet.
v.push_back(1); // acquires property is_non_empty
int i = back(); // required property is present
v.pop_back();   // looses property is_non_empty
</pre>

<p>How do we know when a property is lost? In the most simple case we could say that every mutating (non-const) function called upon our object discards all its properties. But this would imply that in our case even calling <code>back()</code> discards the property, so a more complex way of specifying how the properties are preserved may be required.</p>

<p>So, what do we buy using properties rather than considering any boolean expression such a property? First, properties are pure: they have no side effects. Second, we can express value constraints that are not expressible with expressions:</p>

<pre>
template &lt;typename Iter&gt;
property bool valid_range(Iter begin, Iter end);

template &lt;typename T&gt;
class vector
{
  // invariant: valid_range(this-&gt;begin(), this-&gt;end());
};

template &lt;typename Iter&gt;
void sort(Iter begin, Iter end) // pre: valid_range(this-&gt;begin(), this-&gt;end());
</pre>

<p>We do not propose to add such properties to the language. We just want to show a different perspective on value constraints. N1962 also offers this perspective, but we fear that the focus on broken contract handlers and the order of evaluation of different value constraints, might have obfuscated this idea. A similar effect (as having properties) could be achieved by annotating some value constraints (which use normal expressions) that they must never be evaluated. </p>

<hr>


<h1>Interactions with other C++ features</h1>

<h2>When should precondition hold?</h2>

<p>A common answer to this question is "after function parameters have been initialized, before its first instruction is executed." While it looks good in typical simple examples, consider the following case:</p>

<pre>void fun(Tool * const tool, int const i)
// requires: tool != nullptr
// requires: tool-&gt;value() &gt;= 0;
{
  line_1: int j = transform(i); 
  line_2: int k = process(tool-&gt;value());
}
</pre>

<p>A precondition constrains the state of a remote object of type <code>Tool</code>, its state can change asynchronously. Suppose we can evaluate the precondition upon entering the function. We determine that it is satisfied. We successfully execute instruction <code>line_1</code>, we will now proceed to instruction <code>line_2</code>. It is only now that we really need the precondition to hold, but by now the state of the remote object referred to by pointer <code>tool</code> might have already changed (perhaps function <code>transform</code> modified it)</code>. While the precondition held upon entry, the same condition caused a UB later on. Even if one argues that function must not break the precondition itself, one should still consider the possibility that while executing <code>line_1</code> another thread may concurrently update the object referred to by <code>tool</code>. So, should precondition hold for the entire duration of its function? Or is it a bad idea to express a constrain on remote objects that we do not obtain by value? We do not have a ready answer for this question, but the question is very important, because this kind of interface is used in STL algorithms: they obtain and return iterators, which are handles to remote parts likely aliased and accessible by other threads.</p>

<h2><code>constexpr</code> functions and literal types</h2>

<p><code>constexpr</code> functions can be evaluated at run-time or at compile-time. Ideally, a compile-time evaluation of such function should result in compile-time error when its precondition or postcondition is broken. This is possible if expressions in preconditions and postconditions are core constant expressions. It appears reasonable to require that <code>constexpr</code> functions have only <code>constexpr</code> value constraints.</p>

<p>A similar question needs to be answered for literal types and invariants. For literal types it is required that they have at least one non-copy/move <code>constexpr</code> constructor. This constructor can be used to create a compile-time constant, whose invariant is expected to hold. This may require that the invariant should also be <code>constexpr</code>, as otherwise it is impossible to verify it at compile time. More, some constructors are <code>constexpr</code> even for non-literal classes: their purpose is to guarantee the static initialization of global objects. It is not clear if this should affect the <code>constexpr</code> requirement on the class invariant.</p>

<br>======================================<br>

<h2>Side effects in value constraints</h2>

<p>Value constraints are defined in terms of predicates: functions returning type <code>bool</code>. A notable difference between predicates in the mathematical sense and in an imperative programming language like C++ is that the latter kind of predicates can have side effects. Enforcing the requirement that only const member functions can be invoked does not address the problem of side effects. If we consider the possibility of evaluating these predicates, we have to address the consequences of evaluating these side effects.</p>

<p>One possible approach is to say that it is unspecified whether and which value constraints are evaluated. Additional attributes may to some extent control when we evaluate the predicates. But we guarantee that we do not execute the predicates more than once.</p>

preconditions work only for value semantic use cases (passing by value)


<br>

?? Should evaluating the condition be atomic with function evaluation??
<h1>Constraining STL</h1>

<p>As a usefulness test, any proposed value constraint framework should be checked if it is able to express value constraints in STL. We consider some selected examples.</p> 

<h2>Case 1: a valid range</h2>
<p>This is probably the most difficult one to handle. Nearly any algorithm on STL ranges requires that <code>end</code> is reachable from <code>begin</code>. It cannot be validated by executing any expression. This is why it is required as a precondition: the caller may have better means of ensuring that this requirement is satisfied. For instance, it is obvious that <code>c.begin()</code> and <code>c.end()</code> called on any STL container form a valid range. Is a value constraints framework capable of expressing that the values of <code>c.begin()</code> and <code>c.end()</code> satisfy the requirements of, say, <code>std::for_each</code>?</p>

<h2>Case 2: sub-ranges</h2>
Similarly, in the call <code>m = std::find_if(b, e, p)</code>, assuming that <code>b</code> and <code>e</code> form a valid range, can it be verified or even expressed that <code>b</code> and <code>m</code> as well as <code>m</code> and <code>e</code> form valid ranges and that these combinations should satisfy the requirement of subsequent STL algorithms about valid ranges?


<h2>Case 3: sorting</h2>






<hr />

<h1>Do we need to standardize anything?</h1>




<hr />
avoid giving up control while in an inconsistent state either by calling out, returning, throwing, task switching, etc.<br>
mention psychological arguments
mention structured comments
<hr />

<p>ACSL: http://frama-c.com/download/acsl_1.4.pdf</p>
<p>Can a static analyzer do without pre- post- conditions? -- no, we do not know whose fault.</p>

<p>In this paper we want to change a focus from how broken contracts are responded to in run-time, to how contract violations can be detected at compile-time. What is needed in the language to help such static analysis; and what needs to be standardised as opposed to what should be provided by implementations as "quality of implementation".</p>



<pre>bool is_sorted(iter i, iter j);

void sort(iter i, iter j)
postcondition{ is_sorted(i, j) };

bool binary_search(iter i, iter j);
precondition{ is_sorted(i, j) };
</pre>
 

<pre>bool find(iter i, iter j)
precondition{ is_sorted(i, j) }
{
  return binary_search(i, j);
}

bool find(iter i, iter j)
precondition{ true }
{
  sort(i, j);
  return binary_search(i, j);
}
</pre>


http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2006/n1962.html
</body>
</html>
